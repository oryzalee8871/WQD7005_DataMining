{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "* Weather data significantly affect / correlate to palm oil production yield\n",
    "* Weather data significantly affect / correlate to CPO commodity prices\n",
    "* Palm oil industry related news sentiment significantly affect / correlate to CPO commodity prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub Repo\\\\WQD7005_DataMining\\\\Source Code\\\\03_DataModeling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Relevant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('../../B_Processed_Data/Processed_WeatherData.csv', index_col=0, parse_dates=True).reset_index()\n",
    "station_dict = pd.read_csv('../../A_Raw_Data/WeatherData/Weather_WebCrawl/station_dict.csv')\n",
    "news_df = pd.read_csv('../../B_Processed_Data/Processed_NewsData.csv')\n",
    "production_df = pd.read_csv('../../B_Processed_Data/Processed_CPO_Production.csv')\n",
    "price_df = pd.read_csv('../../B_Processed_Data/Processed_investing_Bursa_CPO_price.csv', index_col=0, parse_dates=True).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weather Data__\n",
    "* Weather data from 26 weather stations in Malaysia were selected.\n",
    "* Data are crawled from <https://en.tutiempo.net>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select weather stations that are relevant\n",
    "station_index=station_dict[station_dict.isna().any(axis=1)]['station_id'].unique()\n",
    "weather_df = weather_df[weather_df['country']=='malaysia'].iloc[:, :-7]\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__News Data__\n",
    "* News data crawled from <www.theedgemarkets.com> using 'Palm Oil' as search keyword.\n",
    "* Sentiment Analysis will be performed on news title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.drop('news_content', axis=1, inplace=True)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CPO Production Data__\n",
    "* Monthly Malaysia CPO production from 2014 to 2020. Data Source: http://mpoc.org.my/monthly-palm-oil-trade-statistics-2014/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CPO Price MYR__\n",
    "* Daily Trading Days CPO Prices on Bursa Malaysia market crawled from <https://investing.com>\n",
    "* CPO price in MYR currency is used to minimize forex effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = price_df.sort_values('Date')\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rename Weather Data Day column to Date__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.rename(columns={'Day':'Date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Standardize Production monthly data timestamp and datatype__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df['Date'] = pd.to_datetime(production_df['Month Year'])\n",
    "production_df = production_df.drop('Month Year', axis=1)\n",
    "production_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert date colume to correct datetime type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['Date'] = pd.to_datetime(news_df['news_date_update'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove any data prior to 2014 August for weather data and news data since our CPO price dataset only starts there__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df[weather_df['Date']>'2014-08']\n",
    "production_df = production_df[production_df['Date']>'2014-08']\n",
    "news_df = news_df[news_df['Date']>'2014-08']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first look at CPO PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,7))\n",
    "price_df.set_index('Date')['Price'].plot(ax=ax)\n",
    "plt.title('Crude Palm Oil (CPO) Price in MYR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then  look at CPO Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CPO Production data seems seasonal with yearly dip near February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,7))\n",
    "production_df.set_index('Date').plot(ax=ax)\n",
    "plt.title('Monthly Malaysia Palm Oil Production')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And look at weather data\n",
    "These are data from 106 weather stations in Malaysia, Indonesia and Singapore. We will only look at 5 main attributes:\n",
    "\n",
    "    1) 'T' : 'Average Temperature (°C)'\n",
    "    2) 'TM': 'Maximum temperature (°C)'\n",
    "    3) 'Tm': 'Minimum temperature (°C)'\n",
    "    4) 'H' : 'Average relative humidity (%)'\n",
    "    5) 'PP': 'Total rainfall and / or snowmelt (mm)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PP: Total rainfall and / or snowmelt (mm)__\n",
    "<br>The data for total rainfall is highly skewed with many centered around zero. We will need additional transformation to normalize this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,7))\n",
    "weather_df['PP'].plot(kind='hist', bins=100, ax=ax)\n",
    "plt.title('PP: Total rainfall and / or snowmelt (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25,7))\n",
    "weather_df.set_index('Date').groupby('station_id')['PP'].plot(ax=ax)\n",
    "plt.title('PP: Total rainfall and / or snowmelt (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Temperature Data__\n",
    "* Temperature data looks reasonable for all weather stations.\n",
    "* Seasonality can be observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25,7))\n",
    "weather_df.set_index('Date').groupby('station_id')['T'].plot()\n",
    "plt.title('T: Average Temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25, 7))\n",
    "weather_df.set_index('Date').groupby('station_id')['TM'].plot()\n",
    "plt.title('TM: Maximum temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25, 7))\n",
    "weather_df.set_index('Date').groupby('station_id')['Tm'].plot()\n",
    "plt.title('Tm: Minimum temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__H: Average relative humidity (%)'__\n",
    "* Humidity looks reasonable with expected seasonality similarly seen from temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (25,7))\n",
    "weather_df.set_index('Date').groupby('station_id')['H'].plot()\n",
    "plt.title('H: Average relative humidity (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPO Price\n",
    "* We need to compute return in longer timeframe so to eliminate noise in daily price fluctuation\n",
    "* Simple Moving Average or trend is computed to be used as a predictor since CPO prices are affected by other wordly components and should be factored into the trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute CPO Price Simple Moving Average as a feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df['Price_MAV5'] = price_df['Price'].rolling(5, min_periods=1).mean()\n",
    "price_df['Vol_MAV5'] = price_df['Vol.'].rolling(5, min_periods=1).mean()\n",
    "price_df['Change_MAV5'] = price_df['Change %'].rolling(5, min_periods=1).mean()\n",
    "price_df['Price_MAV10'] = price_df['Price'].rolling(10, min_periods=1).mean()\n",
    "price_df['Vol_MAV10'] = price_df['Vol.'].rolling(10, min_periods=1).mean()\n",
    "price_df['Change_MAV10'] = price_df['Change %'].rolling(10, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.dropna(inplace=True)\n",
    "price_df = price_df.drop(['Open', 'High', 'Low', 'Vol.', 'Change %'], axis=1)\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPO Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create feature for monthly production change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df = production_df.sort_values('Date')\n",
    "production_df['production_change'] = production_df['Production'].diff()\n",
    "production_df['Month'] = production_df['Date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "\n",
    "* It looks like all the weather data except PP are correlated, but not too strongly around value of 0.6- 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scale column 'PP' in weather data using log(x+1)__\n",
    "* This will make larger values smaller since models perform better with small values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['PP'] = weather_df['PP'].apply(lambda x: math.log(x+1))\n",
    "weather_df['PP'].plot(kind='hist')\n",
    "plt.title(\"log(x+1) for column 'pp'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize Weather Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_attributes = weather_df.iloc[:,-5:-1].columns\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "r=0\n",
    "for i, col in enumerate(w_attributes, 0):\n",
    "    r = i//2\n",
    "    c = i%2\n",
    "    weather_df[col].plot(kind='hist', bins=50, ax=ax[r, c])\n",
    "    ax[r, c].set_title(col)\n",
    "plt.suptitle('Normalized Weather Attributes')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(weather_df.iloc[:,-5:].corr())\n",
    "sns.pairplot(weather_df.iloc[:,-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pivot tables on Weather Data using only T, H and PP__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.set_index('Date', inplace=True)\n",
    "weather_monthly_df = weather_df.groupby([weather_df.index.year.rename('year'), weather_df.index.month.rename('month'), 'station_id']).agg({'T':'mean', 'TM':'mean', 'Tm':'mean', 'H':'mean', 'PP':'sum'})\n",
    "weather_monthly_df.reset_index(inplace=True)\n",
    "weather_monthly_df['Date'] = pd.to_datetime(weather_monthly_df.apply(lambda x: str(x.year)+'-'+str(x.month), axis=1))\n",
    "weather_pivot_df = weather_monthly_df.reset_index().pivot_table(index=['Date'], values=['T', 'H', 'PP'], columns='station_id').ffill().bfill(0)\n",
    "weather_pivot_df.columns = ['_'.join(col) for col in weather_pivot_df.columns.values]\n",
    "\n",
    "weather_pivot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data\n",
    "* From the visualization we can see 'Compound' is computed using 'negative' and 'positive' sentiment attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Perform sentiment analysis using VaderSentiment__\n",
    "<br>1) We need to infer sentiment value on news titles. In order to do this we will use VaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VaderSentiment to perform sentiment analysis on news title\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_df = pd.DataFrame([row for row in news_df['news_title'].apply(lambda x: analyzer.polarity_scores(x))])\n",
    "\n",
    "\n",
    "# Concat date with result, and aggregate mean sentiment score by date\n",
    "sentiment_df = pd.concat([news_df[['Date','news_title']], vader_df], axis=1)\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check Sentiment Result__\n",
    "Following news title make sense to have a negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_df['news_title'].iloc[780])\n",
    "print(sentiment_df[['neg', 'neu', 'pos', 'compound']].iloc[780])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Group sentiment by Date and mean aggregate the sentiment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.groupby('Date').mean().rolling(5, min_periods=1).sum()\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize sentiment attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sentiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all data into respective dataframes\n",
    "We have two sets of merge dataframes here\n",
    "* merge_df is the dataframe prepared for modelling CPO prices using news sentiment, CPO previous price action and CPO production\n",
    "* merge_df2 is the dataframe prepared for modelling CPO production using weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe 1\n",
    "__Merge CPO Price with CPO_production and News Sentiment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and shift 1 month on CPO production to account for the report that usually come at the end of month\n",
    "merge_df = pd.merge(price_df, production_df.drop(['Production', 'Month'], axis=1).shift(1), how='outer', on='Date')\n",
    "merge_df = merge_df[(merge_df['Date']<'2020') & (merge_df['Date']>='2014-11')].sort_values('Date')\n",
    "\n",
    "# Forward Fill Production Change Data\n",
    "merge_df['production_change'] = merge_df['production_change'].ffill()\n",
    "\n",
    "merge_df['month'] = merge_df['Date'].dt.month\n",
    "merge_df['dayofweek'] = merge_df['Date'].dt.dayofweek\n",
    "\n",
    "# Drop rows where price is NA due to the merge\n",
    "merge_df = merge_df[~merge_df['Price'].isna()]\n",
    "\n",
    "# Smooth out the production data using rolling mean of 10 periods\n",
    "merge_df['production_change'] = merge_df['production_change'].rolling(10, min_periods=1).mean()\n",
    "\n",
    "\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the same dataframe to news sentiment\n",
    "merge_df = pd.merge(merge_df, sentiment_df, how='left', on='Date')\n",
    "\n",
    "# forward fill the sentiment if any na\n",
    "merge_df[['neg', 'neu', 'pos', 'compound']] = merge_df[['neg', 'neu', 'pos', 'compound']].ffill()\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe 2\n",
    "__Merge CPO Production data with Weather Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df2 = pd.merge(production_df.drop('Production', axis=1), weather_pivot_df, how='left', on='Date').dropna()\n",
    "merge_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize attributes in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "final_df = merge_df.copy().set_index('Date')\n",
    "final_df.iloc[:, 8:] = scaler.fit_transform(final_df.iloc[:, 8:])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "final_df2 = merge_df2.copy().set_index('Date')\n",
    "scale_columns=set(final_df2.columns)-set(['Date','Month'])\n",
    "final_df2.loc[:, scale_columns] = scaler.fit_transform(final_df2.loc[:, scale_columns])\n",
    "final_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Modeling for CPO Price\n",
    "* We use a simple neural network constructed from tensorflow and keras Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "RANDOM_STATE=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT= int(len(X)*(1-0.33))\n",
    "attributes= ['Price', 'month', 'dayofweek', 'Price_MAV5', 'Price_MAV10', 'Vol_MAV5', 'Change_MAV5', 'production_change', 'pos', 'neu', 'neg', 'compound']\n",
    "multi_data = merge_df.loc[:,attributes]\n",
    "tf.random.set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data.plot(subplots=True, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = multi_data.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
    "\n",
    "# standardize data\n",
    "dataset = (dataset-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "* WE use history window of 20 trading days oto predict the next day CPO price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 20\n",
    "future_target = 1\n",
    "STEP = 1\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 0], 0,\n",
    "                                                   TRAIN_SPLIT, past_history,\n",
    "                                                   future_target, STEP,\n",
    "                                                   single_step=True)\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 0],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)\n",
    "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(64, input_shape=x_train_single.shape[-2:], recurrent_dropout=0.5, activation='relu'))\n",
    "# single_step_model.add(tf.keras.layers.LSTM(64, input_shape=x_train_single.shape[-2:], recurrent_dropout=0.5, activation='relu'))\n",
    "single_step_model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, kernel_initializer='he_normal'))\n",
    "single_step_model.add(tf.keras.layers.Dropout(0.3))\n",
    "single_step_model.add(tf.keras.layers.Dense(1, activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 15\n",
    "EPOCHS = 65\n",
    "single_step_model.compile(optimizer='nadam', loss='mae', metrics=['mae'])\n",
    "history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                validation_data=val_data_single,\n",
    "                                validation_steps=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "print(history.history['loss'][-1], history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "y_pred = pd.Series(single_step_model.predict(x_val_single).flatten(), index=merge_df[TRAIN_SPLIT+20:-1].loc[:,'Date']).rename('Predicted')\n",
    "y_pred = (y_pred*data_std[0])+data_mean[0]\n",
    "plt.plot(merge_df[['Date','Price']].set_index('Date'), label='Actual', color='black')\n",
    "plt.plot(y_pred.index, y_pred, label='Predicted', color = 'darkorange')\n",
    "plt.legend(fontsize=16)\n",
    "plt.title('Multivariate single step LSTM forecast on CPO Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip isntall shap\n",
    "import shap\n",
    "DE = shap.DeepExplainer(single_step_model, x_train_single) # X_train is 3d numpy.ndarray\n",
    "shap_values = DE.shap_values(x_val_single, check_additivity=False) # X_validate is 3d numpy.ndarray\n",
    "\n",
    "shap.initjs()\n",
    "shap.summary_plot(\n",
    "    shap_values[0], \n",
    "    x_val_single,\n",
    "    feature_names=attributes,\n",
    "    max_display=50,\n",
    "    plot_type='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import make_scorer\n",
    "selector= (RFE(single_step_model, n_features_to_select=5, step=1))\n",
    "r = permutation_importance(single_step_model, x_val_single, y_val_single[0], n_repeats=5, scoring=make_scorer(mean_absolute_error), random_state=RANDOM_STATE)\n",
    "print(list(X_train.columns[r.importances_mean.argsort()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(np.abs(r.importances_mean))[-5:]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(5), r.importances_mean[indices], color=\"r\", align=\"center\")\n",
    "plt.yticks(range(5), X.columns[indices])\n",
    "plt.ylim([-1, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Modelling for CPO Production\n",
    "* CPO Production Monthly data is only available since 2014 with a total of 63 observation.\n",
    "* This is quite a limited sample size to work with, thus we are using only simple regressor here for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df2.iloc[:,1:]\n",
    "y = final_df2.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "* Support Vector Regressor\n",
    "* Random Forest Regressor\n",
    "* KNeighbors Regressor\n",
    "* Voting Regressor comprising of above three regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(gamma='auto')\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "knn = KNeighborsRegressor()\n",
    "vote = VotingRegressor([('svr', svr), ('rf', rf), ('knn', knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model using 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RANDOM_STATE = 123\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, ax = plt.subplots(N_SPLITS, 4, figsize=(16,4*N_SPLITS))\n",
    "plt.tight_layout(h_pad=7)\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=False, random_state=RANDOM_STATE)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "    \n",
    "    for j, clf in enumerate([svr, rf, knn]):\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_clf = clf.predict(X_test)\n",
    "    \n",
    "        mse = mean_squared_error(y_pred_clf, y_test)\n",
    "        mae = mean_absolute_error(y_pred_clf, y_test)\n",
    "        plot_data = y_test.rename('actual').to_frame()\n",
    "        plot_data['predicted']=y_pred_clf\n",
    "        plot_data.plot(ax=ax[i, j])\n",
    "        ax[i,j].set_title(f'<{str(clf.__class__).split(\".\")[-1][:-2]}>\\nMSE:{mse}')\n",
    "        \n",
    "    vote.fit(X_train, y_train)\n",
    "    y_pred_vote = vote.predict(X_test)\n",
    "    mse = mean_squared_error(y_pred_vote, y_test)\n",
    "    mae = mean_absolute_error(y_pred_vote, y_test)\n",
    "    plot_data = y_test.rename('actual').to_frame()\n",
    "    plot_data['predicted']=y_pred_vote\n",
    "    plot_data.plot(ax=ax[i, 3])\n",
    "    ax[i,3].set_title(f'<Voting Classifier>\\nMSE:{mse}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance using permutation importance\n",
    "Using the voting classifier as our final model, we look at the top feature recommended by the permutation importance from sklearn.\n",
    "* The top 1 feature is 'Month' which is fairly expected due to the yearly seasonality depicted in the production_change plot. \n",
    "* Most of the other top 10 features are 'PP' and 'H' which are the total rainfall and mean humidity for that month . Some stations are ranked higher than others because certain location have larger palm oil plantation land area thus have higher weightage for the CPO production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(vote, np.asarray(X_test), np.asarray(y_test), n_repeats=30, scoring=make_scorer(mean_absolute_error), random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(np.abs(r.importances_mean))[-10:]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(10), r.importances_mean[indices], color=\"r\", align=\"center\")\n",
    "plt.yticks(range(10), X.columns[indices])\n",
    "plt.ylim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
